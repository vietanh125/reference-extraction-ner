{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 25 16:34:32 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   44C    P8    17W / 260W |    348MiB / 11016MiB |      1%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1236      G   /usr/lib/xorg/Xorg                           139MiB |\n",
      "|    0      4130      G   compiz                                       141MiB |\n",
      "|    0     27162      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files    53MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:          32098        5166       20651         156        6279       26087\n",
      "Swap:           979          90         889\n"
     ]
    }
   ],
   "source": [
    "# %run -i data_process.py\n",
    "!nvidia-smi\n",
    "!free -m\n",
    "# !conda list tf2crf\n",
    "# !pip install seqeval[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-8c5c2fc0600d>:21: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True 2.2.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "import pickle\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tf2crf import CRF\n",
    "from nlp import model_transformer\n",
    "# from tensorflow.compat.v1.keras.backend import set_session\n",
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "# config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "# set_session(sess)\n",
    "print(tf.test.is_gpu_available(), tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_padding(y_test, y_pred, tags2idx):\n",
    "    '''\n",
    "    cắt padding ở cu￿ối m￿ỗi c￿âu trong y_t￿est v￿à y_pred tương ứng\n",
    "\n",
    "    :param y_test: [1, 1, 1, 1, 0, 0, 0, 0]\n",
    "    :param y_pred: [1, 1, 2, 2, 2, 1, 3, 0]\n",
    "    :param tags2idx: {'B-L': 1, 'I-L':2, 'pad':0}\n",
    "\n",
    "    :return:\n",
    "        y_test: [1, 1, 1, 1]\n",
    "        y_pred: [1, 1, 2, 2]\n",
    "    '''\n",
    "    y_pred_trim = []\n",
    "    y_test_trim = []\n",
    "    for i, y in enumerate(y_test):\n",
    "        first_pad_list = np.where(y == tags2idx['pad'])[0]\n",
    "        pred = y_pred[i]\n",
    "        if len(first_pad_list) != 0:\n",
    "            y = y[:first_pad_list[0]]\n",
    "            pred = pred[:first_pad_list[0]]\n",
    "        y_pred_trim.append(pred)\n",
    "        y_test_trim.append(y)\n",
    "    y_test = np.concatenate(y_test_trim).ravel()\n",
    "    y_pred = np.concatenate(y_pred_trim).ravel()\n",
    "    return y_test, y_pred\n",
    "\n",
    "def spit_nested_labels(y):\n",
    "    y_nested = []\n",
    "    for i, row in enumerate(y):\n",
    "        nested = []\n",
    "        for j, x in enumerate(row):\n",
    "            n = x.split('_')\n",
    "            if len(n) > 1:\n",
    "                nested.append(n[1])\n",
    "                y[i][j] = n[0]\n",
    "            else:\n",
    "                nested.append('O')\n",
    "        y_nested.append(nested)\n",
    "    return np.append(y, np.array(y_nested), 1)\n",
    "\n",
    "\n",
    "\n",
    "class F1Callback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        print(\"init\")\n",
    "        self.maps = []\n",
    "\n",
    "    def eval_map(self, X, y):\n",
    "#         x_val, y_true = X, y\n",
    "        y_pred = self.model.predict(X)\n",
    "        y_pred = np.argmax(y_pred, axis=2)\n",
    "        y = np.argmax(y, axis=2)\n",
    "        y_pred = np.vectorize(idx2tags.get)(y_pred)\n",
    "        y = np.vectorize(idx2tags.get)(y)\n",
    "\n",
    "        y = spit_nested_labels(y)\n",
    "        y_pred = spit_nested_labels(y_pred)\n",
    "        score = f1_score(y.tolist(), y_pred.tolist())\n",
    "        del X, y, y_pred\n",
    "        return score\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val = self.eval_map(self.X_val, self.y_val)\n",
    "#         train = self.eval_map(self.X_train, y_train)\n",
    "        print(\" - F1_val: %f\"%(val))\n",
    "        logs['F1_val'] = val\n",
    "        self.maps.append(val)\n",
    "\n",
    "def crop(dimension, start, end, name=None):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "    return Lambda(func, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296, 128, 1891)\n",
      "(1302, 128, 1891)\n",
      "(1336, 128, 1891)\n",
      "(5211, 128, 1891) (5211, 128)\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"X_2.npy\").astype(np.float32)\n",
    "print(X.shape)\n",
    "\n",
    "X2 = np.load(\"X_3.npy\").astype(np.float32)\n",
    "print(X2.shape)\n",
    "X = np.vstack((X, X2))\n",
    "\n",
    "X3 = np.load(\"X_5.npy\").astype(np.float32)\n",
    "print(X3.shape)\n",
    "X = np.vstack((X, X3))\n",
    "\n",
    "X5 = np.load(\"X_4.npy\").astype(np.float32)\n",
    "X = np.vstack((X, X5))\n",
    "\n",
    "y = np.load(\"y_2.npy\")\n",
    "y2 = np.load(\"y_3.npy\")\n",
    "y3 = np.load(\"y_5.npy\")\n",
    "y5 = np.load(\"y_4.npy\")\n",
    "y = np.vstack((y, y2, y3, y5))\n",
    "# X = np.delete(X, np.s_[2048:2348], axis=2)\n",
    "del y3, y2, y5, X2, X3, X5\n",
    "# X = X[:, :, :2024]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags2idx = {'B-HP': 0, 'B-TTLT': 2, 'I-PL_I-PL': 3, 'I-ND_I-TT': 4, 'I-NQ': 5, 'I-L_I-BL': 6, 'I-TTLT': 7, 'B-L': 8, 'I-ND_I-L': 9, \n",
    "            'I-ND_I-ND': 10, 'I-ND_B-TT': 11, 'I-HP': 12, 'I-ND': 13, 'I-TT': 14, 'I-PL': 15, 'I-QD': 16, 'I-L_I-L': 17, 'B-BL': 18, \n",
    "            'I-TT_B-ND': 19, 'I-L_B-ND': 20, 'I-ND_B-L': 21, 'B-ND': 22, 'B-PL': 23, 'I-BL': 24, 'B-QD': 25, 'I-L_I-ND': 26, 'O': 27, \n",
    "            'I-TT_B-TT': 28, 'B-NQ': 29, 'I-L_B-L': 30, 'I-L_B-BL': 31, 'I-TT_I-ND': 32, 'I-TT_I-TT': 33, 'I-L': 34, 'B-TT': 35, \n",
    "            'I-PL_B-PL': 36, 'I-ND_B-ND': 1}\n",
    "y[y=='pad'] = 'O'\n",
    "y = np.vectorize(tags2idx.get)(y)\n",
    "idx2tags = {v: k for k, v in tags2idx.items()}\n",
    "y = np.array([to_categorical(l, num_classes=len(tags2idx.keys())) for l in y])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "del X, y\n",
    "filepath=\"models/branches-{epoch:02d}-{F1_val:.4f}.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='F1_val', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(128, X_train.shape[2]), name='bert_ftext_manual')\n",
    "x = Masking(mask_value=0., input_shape=(128, X_train.shape[2]))(inp)\n",
    "bert = crop(2, 0, 768*2+300, 'split_bert_ftext')(x)\n",
    "char = crop(2, 1841, 1891, 'split_char')(x)\n",
    "x = crop(2, 1836, 1841, 'split_manual_feat')(x)\n",
    "\n",
    "bert = Bidirectional(GRU(200, return_sequences=True, dropout=0.5))(bert)\n",
    "bert = Bidirectional(GRU(200, return_sequences=True, dropout=0.5))(bert)\n",
    "\n",
    "x = TimeDistributed(Dense(32, activation='relu'))(x)\n",
    "\n",
    "char = TimeDistributed(Embedding(230, 30, input_length=50), name='char_embedding')(char)\n",
    "# char = TimeDistributed(Conv1D(kernel_size=7, filters=30, padding='same', activation='tanh', strides=1))(char)\n",
    "# char = TimeDistributed(MaxPooling1D(2))(char)\n",
    "# char = TimeDistributed(Conv1D(kernel_size=5, filters=40, padding='same', activation='tanh', strides=1))(char)\n",
    "# char = TimeDistributed(MaxPooling1D(2))(char)\n",
    "# char = TimeDistributed(Conv1D(kernel_size=3, filters=50, padding='same', activation='tanh', strides=1))(char)\n",
    "# char = TimeDistributed(MaxPooling1D(2))(char)\n",
    "# char = TimeDistributed(Flatten())(char)\n",
    "# char = TimeDistributed(Dropout(0.5))(char)\n",
    "char = TimeDistributed(Bidirectional(GRU(100, return_sequences=True, dropout=0.5)))(char)\n",
    "char = TimeDistributed(Bidirectional(GRU(100, return_sequences=False, dropout=0.5)))(char)\n",
    "\n",
    "char = TimeDistributed(Dense(100, activation='relu'))(char)\n",
    "\n",
    "x = concatenate([bert, x, char])\n",
    "output = Dense(len(tags2idx.keys()), activation=None, name='output')(x)\n",
    "crf = CRF()\n",
    "output = crf(output)\n",
    "\n",
    "model = Model(inp, output)\n",
    "# model = load_model('models/branches-32-0.9573.h5')\n",
    "\n",
    "# model = model_transformer()\n",
    "print(\"Model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "bert_ftext_manual (InputLayer)  [(None, 128, 1891)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 128, 1891)    0           bert_ftext_manual[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "split_char (Lambda)             (None, 128, 50)      0           masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, 128, 50, 30)  6900        split_char[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "split_bert_ftext (Lambda)       (None, 128, 1836)    0           masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 128, 50, 200) 79200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 128, 400)     2445600     split_bert_ftext[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "split_manual_feat (Lambda)      (None, 128, 5)       0           masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 128, 200)     181200      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 400)     722400      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 128, 32)      192         split_manual_feat[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 128, 100)     20100       time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 532)     0           bidirectional_1[0][0]            \n",
      "                                                                 time_distributed[0][0]           \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 128, 37)      19721       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf (CRF)                       (None, 128, 37)      1369        output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,476,682\n",
      "Trainable params: 3,476,682\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "init\n",
      "Epoch 1/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 58.8699 - F1_val: 0.766528\n",
      "\n",
      "Epoch 00001: F1_val improved from -inf to 0.76653, saving model to models/branches-01-0.7665.h5\n",
      "66/66 [==============================] - 30s 459ms/step - loss: 58.8699 - F1_val: 0.7665\n",
      "Epoch 2/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 5.7053 - F1_val: 0.902608\n",
      "\n",
      "Epoch 00002: F1_val improved from 0.76653 to 0.90261, saving model to models/branches-02-0.9026.h5\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 5.7053 - F1_val: 0.9026\n",
      "Epoch 3/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.5300 - F1_val: 0.926820\n",
      "\n",
      "Epoch 00003: F1_val improved from 0.90261 to 0.92682, saving model to models/branches-03-0.9268.h5\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 3.5300 - F1_val: 0.9268\n",
      "Epoch 4/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.8750 - F1_val: 0.929459\n",
      "\n",
      "Epoch 00004: F1_val improved from 0.92682 to 0.92946, saving model to models/branches-04-0.9295.h5\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 2.8750 - F1_val: 0.9295\n",
      "Epoch 5/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.2468 - F1_val: 0.933789\n",
      "\n",
      "Epoch 00005: F1_val improved from 0.92946 to 0.93379, saving model to models/branches-05-0.9338.h5\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 2.2468 - F1_val: 0.9338\n",
      "Epoch 6/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.8118 - F1_val: 0.923193\n",
      "\n",
      "Epoch 00006: F1_val did not improve from 0.93379\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 1.8118 - F1_val: 0.9232\n",
      "Epoch 7/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.5031 - F1_val: 0.933333\n",
      "\n",
      "Epoch 00007: F1_val did not improve from 0.93379\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 1.5031 - F1_val: 0.9333\n",
      "Epoch 8/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3232 - F1_val: 0.947288\n",
      "\n",
      "Epoch 00008: F1_val improved from 0.93379 to 0.94729, saving model to models/branches-08-0.9473.h5\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 1.3232 - F1_val: 0.9473\n",
      "Epoch 9/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1780 - F1_val: 0.941985\n",
      "\n",
      "Epoch 00009: F1_val did not improve from 0.94729\n",
      "66/66 [==============================] - 28s 428ms/step - loss: 1.1780 - F1_val: 0.9420\n",
      "Epoch 10/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0763 - F1_val: 0.930418\n",
      "\n",
      "Epoch 00010: F1_val did not improve from 0.94729\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 1.0763 - F1_val: 0.9304\n",
      "Epoch 11/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8429 - F1_val: 0.941446\n",
      "\n",
      "Epoch 00011: F1_val did not improve from 0.94729\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.8429 - F1_val: 0.9414\n",
      "Epoch 12/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6547 - F1_val: 0.949081\n",
      "\n",
      "Epoch 00012: F1_val improved from 0.94729 to 0.94908, saving model to models/branches-12-0.9491.h5\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.6547 - F1_val: 0.9491\n",
      "Epoch 13/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8793 - F1_val: 0.947409\n",
      "\n",
      "Epoch 00013: F1_val did not improve from 0.94908\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.8793 - F1_val: 0.9474\n",
      "Epoch 14/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5988 - F1_val: 0.950268\n",
      "\n",
      "Epoch 00014: F1_val improved from 0.94908 to 0.95027, saving model to models/branches-14-0.9503.h5\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.5988 - F1_val: 0.9503\n",
      "Epoch 15/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4931 - F1_val: 0.950192\n",
      "\n",
      "Epoch 00015: F1_val did not improve from 0.95027\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.4931 - F1_val: 0.9502\n",
      "Epoch 16/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3966 - F1_val: 0.953314\n",
      "\n",
      "Epoch 00016: F1_val improved from 0.95027 to 0.95331, saving model to models/branches-16-0.9533.h5\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.3966 - F1_val: 0.9533\n",
      "Epoch 17/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3857 - F1_val: 0.951980\n",
      "\n",
      "Epoch 00017: F1_val did not improve from 0.95331\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.3857 - F1_val: 0.9520\n",
      "Epoch 18/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3750 - F1_val: 0.954537\n",
      "\n",
      "Epoch 00018: F1_val improved from 0.95331 to 0.95454, saving model to models/branches-18-0.9545.h5\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.3750 - F1_val: 0.9545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2811 - F1_val: 0.953876\n",
      "\n",
      "Epoch 00019: F1_val did not improve from 0.95454\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 0.2811 - F1_val: 0.9539\n",
      "Epoch 20/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2224 - F1_val: 0.955556\n",
      "\n",
      "Epoch 00020: F1_val improved from 0.95454 to 0.95556, saving model to models/branches-20-0.9556.h5\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.2224 - F1_val: 0.9556\n",
      "Epoch 21/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1920 - F1_val: 0.954920\n",
      "\n",
      "Epoch 00021: F1_val did not improve from 0.95556\n",
      "66/66 [==============================] - 29s 436ms/step - loss: 0.1920 - F1_val: 0.9549\n",
      "Epoch 22/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1852 - F1_val: 0.957638\n",
      "\n",
      "Epoch 00022: F1_val improved from 0.95556 to 0.95764, saving model to models/branches-22-0.9576.h5\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 0.1852 - F1_val: 0.9576\n",
      "Epoch 23/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2167 - F1_val: 0.955304\n",
      "\n",
      "Epoch 00023: F1_val did not improve from 0.95764\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.2167 - F1_val: 0.9553\n",
      "Epoch 24/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2130 - F1_val: 0.954128\n",
      "\n",
      "Epoch 00024: F1_val did not improve from 0.95764\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 0.2130 - F1_val: 0.9541\n",
      "Epoch 25/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2993 - F1_val: 0.957741\n",
      "\n",
      "Epoch 00025: F1_val improved from 0.95764 to 0.95774, saving model to models/branches-25-0.9577.h5\n",
      "66/66 [==============================] - 29s 436ms/step - loss: 0.2993 - F1_val: 0.9577\n",
      "Epoch 26/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3160 - F1_val: 0.953275\n",
      "\n",
      "Epoch 00026: F1_val did not improve from 0.95774\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.3160 - F1_val: 0.9533\n",
      "Epoch 27/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2600 - F1_val: 0.954641\n",
      "\n",
      "Epoch 00027: F1_val did not improve from 0.95774\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.2600 - F1_val: 0.9546\n",
      "Epoch 28/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1932 - F1_val: 0.955235\n",
      "\n",
      "Epoch 00028: F1_val did not improve from 0.95774\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.1932 - F1_val: 0.9552\n",
      "Epoch 29/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1237 - F1_val: 0.957990\n",
      "\n",
      "Epoch 00029: F1_val improved from 0.95774 to 0.95799, saving model to models/branches-29-0.9580.h5\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.1237 - F1_val: 0.9580\n",
      "Epoch 30/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1386 - F1_val: 0.952145\n",
      "\n",
      "Epoch 00030: F1_val did not improve from 0.95799\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.1386 - F1_val: 0.9521\n",
      "Epoch 31/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1525 - F1_val: 0.956872\n",
      "\n",
      "Epoch 00031: F1_val did not improve from 0.95799\n",
      "66/66 [==============================] - 29s 438ms/step - loss: 0.1525 - F1_val: 0.9569\n",
      "Epoch 32/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0800 - F1_val: 0.956973\n",
      "\n",
      "Epoch 00032: F1_val did not improve from 0.95799\n",
      "66/66 [==============================] - 29s 436ms/step - loss: 0.0800 - F1_val: 0.9570\n",
      "Epoch 33/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1002 - F1_val: 0.959693\n",
      "\n",
      "Epoch 00033: F1_val improved from 0.95799 to 0.95969, saving model to models/branches-33-0.9597.h5\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.1002 - F1_val: 0.9597\n",
      "Epoch 34/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1323 - F1_val: 0.957687\n",
      "\n",
      "Epoch 00034: F1_val did not improve from 0.95969\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.1323 - F1_val: 0.9577\n",
      "Epoch 35/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0957 - F1_val: 0.958357\n",
      "\n",
      "Epoch 00035: F1_val did not improve from 0.95969\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.0957 - F1_val: 0.9584\n",
      "Epoch 36/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0831 - F1_val: 0.957006\n",
      "\n",
      "Epoch 00036: F1_val did not improve from 0.95969\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.0831 - F1_val: 0.9570\n",
      "Epoch 37/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0663 - F1_val: 0.962735\n",
      "\n",
      "Epoch 00037: F1_val improved from 0.95969 to 0.96274, saving model to models/branches-37-0.9627.h5\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 0.0663 - F1_val: 0.9627\n",
      "Epoch 38/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0972 - F1_val: 0.951597\n",
      "\n",
      "Epoch 00038: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.0972 - F1_val: 0.9516\n",
      "Epoch 39/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2393 - F1_val: 0.958173\n",
      "\n",
      "Epoch 00039: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.2393 - F1_val: 0.9582\n",
      "Epoch 40/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2316 - F1_val: 0.955235\n",
      "\n",
      "Epoch 00040: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.2316 - F1_val: 0.9552\n",
      "Epoch 41/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1962 - F1_val: 0.957222\n",
      "\n",
      "Epoch 00041: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 438ms/step - loss: 0.1962 - F1_val: 0.9572\n",
      "Epoch 42/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1160 - F1_val: 0.955888\n",
      "\n",
      "Epoch 00042: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 443ms/step - loss: 0.1160 - F1_val: 0.9559\n",
      "Epoch 43/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0650 - F1_val: 0.958909\n",
      "\n",
      "Epoch 00043: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 438ms/step - loss: 0.0650 - F1_val: 0.9589\n",
      "Epoch 44/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0889 - F1_val: 0.957557\n",
      "\n",
      "Epoch 00044: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.0889 - F1_val: 0.9576\n",
      "Epoch 45/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0968 - F1_val: 0.959815\n",
      "\n",
      "Epoch 00045: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.0968 - F1_val: 0.9598\n",
      "Epoch 46/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0512 - F1_val: 0.959862\n",
      "\n",
      "Epoch 00046: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 0.0512 - F1_val: 0.9599\n",
      "Epoch 47/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0364 - F1_val: 0.958541\n",
      "\n",
      "Epoch 00047: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.0364 - F1_val: 0.9585\n",
      "Epoch 48/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1544 - F1_val: 0.959524\n",
      "\n",
      "Epoch 00048: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.1544 - F1_val: 0.9595\n",
      "Epoch 49/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1242 - F1_val: 0.956272\n",
      "\n",
      "Epoch 00049: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.1242 - F1_val: 0.9563\n",
      "Epoch 50/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0969 - F1_val: 0.959893\n",
      "\n",
      "Epoch 00050: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 438ms/step - loss: 0.0969 - F1_val: 0.9599\n",
      "Epoch 51/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0907 - F1_val: 0.953221\n",
      "\n",
      "Epoch 00051: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.0907 - F1_val: 0.9532\n",
      "Epoch 52/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1345 - F1_val: 0.957304\n",
      "\n",
      "Epoch 00052: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 438ms/step - loss: 0.1345 - F1_val: 0.9573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1084 - F1_val: 0.954076\n",
      "\n",
      "Epoch 00053: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 0.1084 - F1_val: 0.9541\n",
      "Epoch 54/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0875 - F1_val: 0.958022\n",
      "\n",
      "Epoch 00054: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.0875 - F1_val: 0.9580\n",
      "Epoch 55/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0641 - F1_val: 0.958038\n",
      "\n",
      "Epoch 00055: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.0641 - F1_val: 0.9580\n",
      "Epoch 56/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0616 - F1_val: 0.958054\n",
      "\n",
      "Epoch 00056: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 438ms/step - loss: 0.0616 - F1_val: 0.9581\n",
      "Epoch 57/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0319 - F1_val: 0.960061\n",
      "\n",
      "Epoch 00057: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.0319 - F1_val: 0.9601\n",
      "Epoch 58/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0297 - F1_val: 0.957838\n",
      "\n",
      "Epoch 00058: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.0297 - F1_val: 0.9578\n",
      "Epoch 59/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0190 - F1_val: 0.958988\n",
      "\n",
      "Epoch 00059: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.0190 - F1_val: 0.9590\n",
      "Epoch 60/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1027 - F1_val: 0.956672\n",
      "\n",
      "Epoch 00060: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.1027 - F1_val: 0.9567\n",
      "Epoch 61/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1084 - F1_val: 0.956139\n",
      "\n",
      "Epoch 00061: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.1084 - F1_val: 0.9561\n",
      "Epoch 62/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1392 - F1_val: 0.959293\n",
      "\n",
      "Epoch 00062: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.1392 - F1_val: 0.9593\n",
      "Epoch 63/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1964 - F1_val: 0.943497\n",
      "\n",
      "Epoch 00063: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.1964 - F1_val: 0.9435\n",
      "Epoch 64/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1263 - F1_val: 0.958878\n",
      "\n",
      "Epoch 00064: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.1263 - F1_val: 0.9589\n",
      "Epoch 65/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1130 - F1_val: 0.958957\n",
      "\n",
      "Epoch 00065: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.1130 - F1_val: 0.9590\n",
      "Epoch 66/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0719 - F1_val: 0.955888\n",
      "\n",
      "Epoch 00066: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.0719 - F1_val: 0.9559\n",
      "Epoch 67/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1414 - F1_val: 0.954476\n",
      "\n",
      "Epoch 00067: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.1414 - F1_val: 0.9545\n",
      "Epoch 68/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2969 - F1_val: 0.950726\n",
      "\n",
      "Epoch 00068: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.2969 - F1_val: 0.9507\n",
      "Epoch 69/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1830 - F1_val: 0.956221\n",
      "\n",
      "Epoch 00069: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.1830 - F1_val: 0.9562\n",
      "Epoch 70/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0929 - F1_val: 0.957105\n",
      "\n",
      "Epoch 00070: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.0929 - F1_val: 0.9571\n",
      "Epoch 71/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0606 - F1_val: 0.961568\n",
      "\n",
      "Epoch 00071: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 441ms/step - loss: 0.0606 - F1_val: 0.9616\n",
      "Epoch 72/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0379 - F1_val: 0.960123\n",
      "\n",
      "Epoch 00072: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 444ms/step - loss: 0.0379 - F1_val: 0.9601\n",
      "Epoch 73/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0348 - F1_val: 0.962166\n",
      "\n",
      "Epoch 00073: F1_val did not improve from 0.96274\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.0348 - F1_val: 0.9622\n",
      "Epoch 74/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0217 - F1_val: 0.963832\n",
      "\n",
      "Epoch 00074: F1_val improved from 0.96274 to 0.96383, saving model to models/branches-74-0.9638.h5\n",
      "66/66 [==============================] - 29s 442ms/step - loss: 0.0217 - F1_val: 0.9638\n",
      "Epoch 75/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0404 - F1_val: 0.962536\n",
      "\n",
      "Epoch 00075: F1_val did not improve from 0.96383\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 0.0404 - F1_val: 0.9625\n",
      "Epoch 76/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0686 - F1_val: 0.958373\n",
      "\n",
      "Epoch 00076: F1_val did not improve from 0.96383\n",
      "66/66 [==============================] - 29s 438ms/step - loss: 0.0686 - F1_val: 0.9584\n",
      "Epoch 77/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0808 - F1_val: 0.962735\n",
      "\n",
      "Epoch 00077: F1_val did not improve from 0.96383\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.0808 - F1_val: 0.9627\n",
      "Epoch 78/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0641 - F1_val: 0.958589\n",
      "\n",
      "Epoch 00078: F1_val did not improve from 0.96383\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.0641 - F1_val: 0.9586\n",
      "Epoch 79/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0529 - F1_val: 0.957124\n",
      "\n",
      "Epoch 00079: F1_val did not improve from 0.96383\n",
      "66/66 [==============================] - 29s 440ms/step - loss: 0.0529 - F1_val: 0.9571\n",
      "Epoch 80/80\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0523 - F1_val: 0.959540\n",
      "\n",
      "Epoch 00080: F1_val did not improve from 0.96383\n",
      "66/66 [==============================] - 29s 437ms/step - loss: 0.0523 - F1_val: 0.9595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fda907c9350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Adam()\n",
    "model.compile(loss=crf.loss, optimizer=opt)\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "callback = F1Callback(X_val, y_val)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=80, batch_size=64, shuffle=True, callbacks=[callback, checkpoint])\n",
    "# model.save('models/model_2bert.h5')\n",
    "\n",
    "# # model = load_model('model.h5', custom_objects={'CRF': CRF}, compile=False)\n",
    "\n",
    "# del X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1303, 128, 1891)\n",
      "branches-74-0.9638.h5\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "        L     0.9461    0.9473    0.9467      1613\n",
      "       ND     0.9592    0.9673    0.9632       704\n",
      "       TT     0.9156    0.9313    0.9234       233\n",
      "       HP     0.9863    1.0000    0.9931        72\n",
      "       NQ     0.8281    0.8281    0.8281        64\n",
      "       BL     0.9511    0.9511    0.9511       327\n",
      "       QD     0.8901    0.9529    0.9205        85\n",
      "       PL     0.8732    0.8611    0.8671        72\n",
      "     TTLT     1.0000    0.8333    0.9091         6\n",
      "\n",
      "micro avg     0.9427    0.9477    0.9452      3176\n",
      "macro avg     0.9428    0.9477    0.9452      3176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags2idx = {'B-HP': 0, 'B-TTLT': 2, 'I-PL_I-PL': 3, 'I-ND_I-TT': 4, 'I-NQ': 5, 'I-L_I-BL': 6, 'I-TTLT': 7, 'B-L': 8, 'I-ND_I-L': 9, \n",
    "            'I-ND_I-ND': 10, 'I-ND_B-TT': 11, 'I-HP': 12, 'I-ND': 13, 'I-TT': 14, 'I-PL': 15, 'I-QD': 16, 'I-L_I-L': 17, 'B-BL': 18, \n",
    "            'I-TT_B-ND': 19, 'I-L_B-ND': 20, 'I-ND_B-L': 21, 'B-ND': 22, 'B-PL': 23, 'I-BL': 24, 'B-QD': 25, 'I-L_I-ND': 26, 'O': 27, \n",
    "            'I-TT_B-TT': 28, 'B-NQ': 29, 'I-L_B-L': 30, 'I-L_B-BL': 31, 'I-TT_I-ND': 32, 'I-TT_I-TT': 33, 'I-L': 34, 'B-TT': 35, \n",
    "            'I-PL_B-PL': 36, 'I-ND_B-ND': 1}\n",
    "idx2tags = {v: k for k, v in tags2idx.items()}\n",
    "\n",
    "X = np.load(\"X_1.npy\").astype(np.float32)\n",
    "# X = np.delete(X, np.s_[2048:2348], axis=2)\n",
    "print(X.shape)\n",
    "# X3 = np.load(\"X_5.npy\").astype(np.float32)\n",
    "# print(X3.shape)\n",
    "# X = np.vstack((X, X3))\n",
    "y = np.load(\"y_1.npy\")\n",
    "# y4 = np.load(\"y_5.npy\")\n",
    "# # y5 = np.load(\"y_5.npy\")\n",
    "# y = np.vstack((y, y4))\n",
    "y[y=='pad'] = 'O'\n",
    "def eval(X, y, model):\n",
    "    y_pred = np.argmax(model.predict(X), axis=2)\n",
    "    y_pred = np.vectorize(idx2tags.get)(y_pred)\n",
    "    y_test = spit_nested_labels(y)\n",
    "    y_pred = spit_nested_labels(y_pred)\n",
    "    print(classification_report(y_test.tolist(), y_pred.tolist(), digits=4))\n",
    "    \n",
    "import os\n",
    "path = \"models\"\n",
    "ls = os.listdir(path)\n",
    "ls = [\"branches-74-0.9638.h5\"]\n",
    "for name in ls:\n",
    "    if name.split('.')[-1] == 'h5':\n",
    "        model.load_weights(path + '/' + name)\n",
    "        print(name)\n",
    "        eval(X, y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vietanh",
   "language": "python",
   "name": "vietanh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
